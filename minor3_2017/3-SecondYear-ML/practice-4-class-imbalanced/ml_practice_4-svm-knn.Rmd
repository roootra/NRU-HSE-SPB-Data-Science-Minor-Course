---
title: "Об алгоритмах подробнее"
output: html_document
---

```{r, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
library(caret)
```

###  Данные (рак предстательной железы)

Загружаем данные
```{r, warning=FALSE, message=FALSE}
prCancer <- read.csv("~/shared/minor3_2017/3-SecondYear-ML/practice-4-class-imbalanced/Prostate_Cancer.csv")
```

Посмотрим на них

```{r, warning=FALSE, message=FALSE}
str(prCancer)
```

У нас есть 8 параметров, характеризующих новообразование:

* Radius
* Texture
* Perimeter
* Area
* Smoothness
* Compactness
* Symmetry
* Fractal dimension

Переменная `diagnosis_result`, соответствующая диагнозу, имеет два значения:

* В = Benign (доброкачественное новообразование)
* M = Malignant (злокачественное новообразование)

## Обучающая и тестовая выборки

```{r message = F, warning=FALSE}
set.seed(345)
pc.test.ind = createDataPartition(prCancer$diagnosis_result, p = .7, 
                                  list = FALSE, 
                                  times = 1)
pc.test = prCancer[-pc.test.ind,]
pc.main = prCancer[pc.test.ind,]
```

Рассмотрим сначала всего два предиктора -- периметр `perimeter` и коэффицент симметрии `symmetry` 

```{r, warning=FALSE, message=FALSE}
ggplot(data = prCancer, aes(color = diagnosis_result)) + geom_point(aes(x=perimeter, y = symmetry))
data.main <- dplyr::select(pc.main, diagnosis_result, perimeter, symmetry)
data.test <- dplyr::select(pc.test, diagnosis_result, perimeter, symmetry)
```

## Метод k ближайших соседей (k nearest neighbours)

Сначала рассмотрим общий принцип работы. Пусть у нас есть небольшой датасет с двумя классами элементов

```{r echo = FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
x<-c(2,4,4,7,7,9,11,12,14,15,13)
y<-c(6,5,9,7,10,3,2,5,3,5,7)
type<-as.factor(c(1,1,1,1,1,2,2,2,2,2,2))
d <- data.frame(x,y,type)
g <- ggplot(data = d, aes(x=x, y=y)) + 
  geom_point(aes(color = type, shape = type), size = 5) +
  theme(legend.position = "none")
g

```


**1 ближайший сосед:** точка принадлежит к тому классу, что и ближайшая к ней точка. Ближайшая = расположенная на наименьшем расстоянии (расстояние чаще всего Евклидово, т.е. $\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}$).

```{r echo=FALSE}
g + geom_point(x = 9, y = 8, size = 5, shape = 15) +
  geom_segment(x = 9, y = 8, xend = 7, yend = 7, linetype = 2)
```

Итоговое разделение:

```{r echo = F, message=FALSE, warning=FALSE}
library(mlr)
task1 = makeClassifTask(id = "example", data = d, target = "type")
plotLearnerPrediction(learner = "classif.knn", task = task1) + 
  theme(legend.position = "none") + ggtitle("")
```

**k ближайших соседей:** точка принадлежит к тому классу, что и большинство из k ближайших к ней точек. Т.е. ищем k точек, расстояние до которых меньше, считаем долю каждого из классов среди этих точек и выбираем тот класс, доля которого больше. Например, для  k = 3:

```{r echo=FALSE}
g + geom_point(x = 4, y = 7, size = 5, shape = 15, color = "green") +
  geom_segment(x = 4, y = 7, xend = 4, yend = 5, color = "green", linetype = 2) +
  geom_segment(x = 4, y = 7, xend = 4, yend = 9, color = "green", linetype = 2) +
  geom_segment(x = 4, y = 7, xend = 2, yend = 6, color = "green", linetype = 2) +
  geom_point(x = 9, y = 5, size = 5, shape = 15, color = "blue") +
  geom_segment(x = 9, y = 5, xend = 7, yend = 7, color = "blue", linetype = 2) +
  geom_segment(x = 9, y = 5, xend = 9, yend = 3, color = "blue", linetype = 2) +
  geom_segment(x = 9, y = 5, xend = 12, yend = 5, color = "blue", linetype = 2) 
```

Для зеленой точки -- из 3 ближайших соседей 3 красных и 0 голубых, значит относим к красным.

Для синей точки -- из 3 ближайших соседей 1 красная и 2 голубых, значит относим к голубым.

Итоговое разделение:

```{r echo = F, message=FALSE}
plotLearnerPrediction(learner = makeLearner("classif.knn", k=3), task = task1) + 
  theme(legend.position = "none") + ggtitle("")
```

Что будет, если установить k = 11?

Как меняется результат при **увеличении k**? Рассмотрим на немного большем примере

```{r}
iris <- filter(iris, Species != "setosa")
ggplot(data = iris) + geom_point(aes(x=Sepal.Width, y = Petal.Width, color = Species))
```

Для одного соседа:

```{r echo = F, message=FALSE}
task.iris = makeClassifTask(id = "iris", data = iris, target = "Species")
plotLearnerPrediction(learner = "classif.knn", task = task.iris,
                      features = c("Sepal.Width", "Petal.Width")) + 
  ggtitle("")
```

Для 3 соседей:

```{r echo = F, message=FALSE}
plotLearnerPrediction(learner = makeLearner("classif.knn", k=3), task = task.iris,
                      features = c("Sepal.Width", "Petal.Width")) + 
  ggtitle("")
```

Для 5 соседей:

```{r echo = F, message=FALSE}
plotLearnerPrediction(learner = makeLearner("classif.knn", k=5), task = task.iris,
                      features = c("Sepal.Width", "Petal.Width")) + 
  ggtitle("")
```

Для 10 соседей:

```{r echo = F, message=FALSE}
plotLearnerPrediction(learner = makeLearner("classif.knn", k=10), task = task.iris,
                      features = c("Sepal.Width", "Petal.Width")) + 
  ggtitle("")
```

Для 25 соседей:

```{r echo = F, message=FALSE}
plotLearnerPrediction(learner = makeLearner("classif.knn", k=25), task = task.iris,
                      features = c("Sepal.Width", "Petal.Width")) + 
  ggtitle("")
```

Чем больше k - тем более общую модель мы получаем. При небольших k разделение на классы больше "подогнано" к обучающим данным - при k = 1 точность на обучающей выборке равна 1 (ближайшая точка к любой точке -- она сама), но на тестовой она уже существенно ниже. В целом зависимости можно представить следующим образом

Ошибка на обучающей выборке при изменении k
![](http://www.analyticsvidhya.com/wp-content/uploads/2014/10/training-error.png)


Ошибка на тестовой выборке при изменении k
![](http://www.analyticsvidhya.com/wp-content/uploads/2014/10/training-error_11.png)


## Построение моделей

1 ближайший сосед

```{r message = F, warning=FALSE}
library(class)
knn_model <- knn(train = dplyr::select(data.main, -diagnosis_result),
                 test = dplyr::select(data.test, -diagnosis_result),
                 cl=data.main$diagnosis_result)
```

Результаты на тестовой выборке

```{r}
confusionMatrix(knn_model, data.test$diagnosis_result)
```

График

```{r echo = F, message=FALSE}
library(mlr)
task.pc = makeClassifTask(id = "pc", data = prCancer, target = "diagnosis_result")
plotLearnerPrediction(learner = "classif.knn", task = task.pc,
                      features = c("perimeter", "symmetry")) 
```

3 ближайших соседа

```{r message = F, warning=FALSE}
knn_model <- knn(train = dplyr::select(data.main, -diagnosis_result),
                 test = dplyr::select(data.test, -diagnosis_result),
                 cl=data.main$diagnosis_result, k = 3)
```

Результаты на тестовой выборке

```{r}
confusionMatrix(knn_model,data.test$diagnosis_result)
```

График

```{r echo = F, message=FALSE}
plotLearnerPrediction(learner = makeLearner("classif.knn", k=3), task = task.pc,
                      features = c("perimeter", "symmetry")) 
```

### Нормализация

**Обратите внимание:** используемые нами переменные имеют очень разные значения

```{r}
summary(prCancer$perimeter)
summary(prCancer$symmetry)
```

Поэтому вклад в измерение расстояния у них очень разный: разница между самыми отдаленными значениями симметрии `r 0.3040-0.1350`, а периметра -- 120. Соответственно, для алгоритма при вычислении расстояния между точками периметр важнее (это видно и на графике -- почти все разделение вертикальными линиями). Чтобы этого избежать, проводят нормализацию, т.е. приведение к единой шкале (традиционно к значениям от 0 до 1).
Для этого нужно из каждой переменной вычесть ее минимальное значение и разделить на диапазон значений.

```{r}
smin <- min(prCancer$symmetry)
pmin <- min(prCancer$perimeter)
smax <- max(prCancer$symmetry)
pmax <- max(prCancer$perimeter)
prCancer$symmetry <- 
  (prCancer$symmetry - smin)/(smax-smin)
prCancer$perimeter <- 
  (prCancer$perimeter - pmin)/(pmax-pmin)
```

И не забыть повторить разделение на тестовую и обучающую выборки

```{r}
pc.test = prCancer[-pc.test.ind,]
pc.main = prCancer[pc.test.ind,]
```

```{r, warning=FALSE, message=FALSE}
ggplot(data = prCancer, aes(color = diagnosis_result)) + geom_point(aes(x=perimeter, y = symmetry))
data.main <- dplyr::select(pc.main, diagnosis_result, perimeter, symmetry)
data.test <- dplyr::select(pc.test, diagnosis_result, perimeter, symmetry)
```

Новая модель

```{r message = F, warning=FALSE}
knn_model <- knn(train = dplyr::select(data.main, -diagnosis_result),
                 test = dplyr::select(data.test, -diagnosis_result),
                 cl=data.main$diagnosis_result, k=3)
```

И новые результаты на тестовой выборке

```{r}
confusionMatrix(knn_model,data.test$diagnosis_result)
```

График

```{r echo = F, message=FALSE}
task.pc = makeClassifTask(id = "pc", data = prCancer, target = "diagnosis_result")
plotLearnerPrediction(learner = makeLearner("classif.knn", k=3), task = task.pc,
                      features = c("perimeter", "symmetry")) 
```

Результаты для другой реализации метода

```{r}
knn_model2 <- knn3(diagnosis_result~., data = data.main)
knn_pred <- predict(knn_model2, dplyr::select(data.test, -diagnosis_result), type = "class")
confusionMatrix(knn_pred, data.test$diagnosis_result)
```

Для полной картины (и для удобного использования кросс-валидации, например) посмотрим, как такую модель можно построить с помощью функций пакета `caret`. 

```{r}
data.m = data.main
knn_model3 <- caret::train(diagnosis_result ~ ., data=data.main, method = "knn")
knn_model3$finalModel
knn_pred <- predict(knn_model3, data.test)
confusionMatrix(knn_pred, data.test$diagnosis_result)

# вариант с автоматическим масштабированием -- удобно, когда много переменных
knn_model3 <- caret::train(diagnosis_result ~ ., data=data.main, method = "knn", preProcess = c("scale", "center"))
# !! внимание -- для новых данных нужно применять ту же нормализацию, что caret делает автоматически при использовании результата функции train
knn_pred <- predict(knn_model3, data.test)

```

**Ваша очередь**:

* Какой прозноз согласно модели будет при значениях периметра 80 и симметрии 0.22? (для модели knn3)

* Улучшится ли точность, если увеличить k? Какое значение k лучше выбрать?
* Улучшится ли точность, если добавить остальные предикторы?
* Сравните результаты при нормализации и без нее при включении всех предикторов

P.S.
```{r eval = F}
x <- data.frame(symmetry=(0.22-smin)/(smax-smin), perimeter=(80-pmin)/(pmax-pmin))
predict(knn_model2, x)
```


## Метод опорных векторов (Support Vector Machine)

Рассмотрим очень маленький пример: предсказание пола человека по его весу и росту (частично по [этим](http://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/) и [этим](http://www.algorithmist.ru/2011/07/support-vector-machines-with-examples.html) материалам)

Предлагаю собрать свой такой же датасет https://goo.gl/forms/S1jWAaOdQmANklfi2

![](http://i2.wp.com/www.svm-tutorial.com/wp-content/uploads/2014/11/01_svm-dataset1.png
)

Можем разделить данные линией -- все случаи, оказавшиеся ниже линии, будем относить к женщинам, выше -- к мужчинам.

![](http://i2.wp.com/www.svm-tutorial.com/wp-content/uploads/2014/11/01_svm-dataset1-separated.png)

Но таких линий можно провести много:

![](http://i1.wp.com/www.svm-tutorial.com/wp-content/uploads/2014/11/01_svm-dataset1-separated-2.png)

Как решить, какая лучше?

Если мы выбираем линию, которая лежит близко к одному из классов, то можем столкнуться с такой ситуацией для новых данных 

![](http://i2.wp.com/www.svm-tutorial.com/wp-content/uploads/2014/11/01_svm-dataset1-separated-bad.png)

Т.е. нам нужна линия, которая максимально удалена от каждого из классов. Иными словами, линия, для которой расстояние между ней и ближайшими точками из каждого класса максимально.

![](http://i1.wp.com/www.svm-tutorial.com/wp-content/uploads/2014/11/07_withMidpointsAndSeparator.png).

Эти ближайшие точки и называются **опорными векторами**. Такая линия одна и ее можно найти из системы уравнений.

Если у нас больше двух предикторов, принцип сохраняется, только делим мы классы не линией, а плоскостью (или гиперплоскостью).

Но что делать, если линейно мы разделить никак не можем? Например, в такой ситуации:

![](http://1.bp.blogspot.com/-6MPJFSzBl3w/Tgocu23l2tI/AAAAAAAABmc/IcV_LBweKuE/s1600/svm4.png)

Тогда мы с помощью определенных функций преобразуем наши данные, "добавляем им размерность", так, чтобы в новой форме данные можно было разделить линейно. Примерно так: [смотреть](https://www.youtube.com/watch?v=9NrALgHFwTo)

Такие функции делятся на классы -- ядра (kernels), могут быть полиномиальными, радиальными, сигмоидными.

## Построение моделей

Используем нормализованный датасет

Как можно разделить классы? С каким ядром лучше?

### Метод опорных векторов с линейным ядром (Support Vector Machine with linear kernel)

```{r message = F, warning=FALSE}
library("e1071")
svm_model <- svm(diagnosis_result ~ ., data=data.main, kernel="linear")
summary(svm_model)
plot(svm_model, data=data.main)
```

Результаты на обучающей
```{r}

svm.Pred<-predict(svm_model, data.main, probability=FALSE)
confusionMatrix(svm.Pred,data.main$diagnosis_result)
```

и тестовой выборках

```{r}
svm.Pred<-predict(svm_model, data.test, probability=FALSE)
confusionMatrix(svm.Pred,data.test$diagnosis_result)

```




### Метод опорных векторов с полиномиальным ядром (Support Vector Machine with polynomial kernel)

```{r message = F, warning=FALSE}
svm_model <- svm(diagnosis_result ~ ., data=data.main, kernel="polynomial")
summary(svm_model)
plot(svm_model, data=data.main)
```

Результаты на обучающей 

```{r}
svm.Pred<-predict(svm_model, data.main, probability=FALSE)
confusionMatrix(svm.Pred,data.main$diagnosis_result)
```

и тестовой выборках

```{r}
svm.Pred<-predict(svm_model, data.test, probability=FALSE)
confusionMatrix(svm.Pred,data.test$diagnosis_result)

```

Для полной картины (и для удобного использования кросс-валидации, например) посмотрим, как такую модель можно построить с помощью функций пакета `caret`. Напомним, что эти функции -- обертки, они используют реализации из существующих пакетов. Например, SVM с линейным ядром из пакета `kernlab` можно построить, используя method = "svmLinear". А с радиальным method = "svmRadial" (пакет `kernlab`), с полиномиальным method = "svmPoly" (пакет `kernlab`)

```{r warning = F}
library(kernlab)
svm_model0 <- caret::train(factor(diagnosis_result) ~ ., data=data.main, method = 'svmPoly')
svm_model0$finalModel
plot(svm_model0$finalModel)
```

### Метод опорных векторов с радиальным ядром (Support Vector Machine with radial kernel)

```{r message = F, warning=FALSE}
svm_model <- svm(diagnosis_result ~ ., data=data.main, kernel="radial")
summary(svm_model)
plot(svm_model, data=data.main)
```

Результаты на обучающей 

```{r}

svm.Pred<-predict(svm_model, data.main, probability=FALSE)
confusionMatrix(svm.Pred,data.main$diagnosis_result)
```

и тестовой выборках

```{r}
svm.Pred<-predict(svm_model, data.test, probability=FALSE)
confusionMatrix(svm.Pred,data.test$diagnosis_result)

```

### Метод опорных векторов с сигмоидным ядром (Support Vector Machine with sigmoid kernel)

```{r message = F, warning=FALSE}
svm_model <- svm(diagnosis_result ~ ., data=data.main, kernel="sigmoid")
summary(svm_model)
plot(svm_model, data=data.main)
```

Результаты на обучающей 

```{r}

svm.Pred<-predict(svm_model, data.main, probability=FALSE)
confusionMatrix(svm.Pred,data.main$diagnosis_result)
```

и тестовой выборках

```{r}
svm.Pred<-predict(svm_model, data.test, probability=FALSE)
confusionMatrix(svm.Pred,data.test$diagnosis_result)

```

При выборе ядра мы определяем класс функций. Как выбрать ее конкретные параметры?

```{r}
svm_tune <- tune.svm(diagnosis_result ~ ., data=data.main, kernel="sigmoid",
                     cost=10^(-2:2), gamma=c(0.1,0.5,1,2), coef0 = -2:2)
print(svm_tune)
```

Модель с новыми параметрами

```{r message = F, warning=FALSE}
svm_model <- svm(diagnosis_result ~ ., data=data.main, kernel="sigmoid", 
                 gamma = as.numeric(svm_tune$best.parameters["gamma"]),
                 cost = as.numeric(svm_tune$best.parameters["cost"]),
                 coef0 = as.numeric(svm_tune$best.parameters["coef0"]))
summary(svm_model)
plot(svm_model, data=data.main)
```

Результаты на обучающей

```{r}

svm.Pred<-predict(svm_model, data.main, probability=FALSE)
confusionMatrix(svm.Pred,data.main$diagnosis_result)
```

и тестовой выборках

```{r}
svm.Pred<-predict(svm_model, data.test, probability=FALSE)
confusionMatrix(svm.Pred,data.test$diagnosis_result)

```


**Ваша очередь**:

* Улучшится ли точность, если добавить остальные предикторы?
