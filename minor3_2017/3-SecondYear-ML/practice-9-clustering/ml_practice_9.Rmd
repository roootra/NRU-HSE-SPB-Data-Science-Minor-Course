---
title: "ML practice 9: Clustering"
output: html_document
---

В прошлый раз мы говорили про снижение размерности. Снижение размерности  -- это один из примеров обучения без учителя. Мы примерно представляем, чего мы хотим добиться, но у нас нет правильного ответа. 
Сегодня мы поговорим о другой задаче обучения без учителя, а именно о кластеризации.

Но прежде чем мы начнем, ответьте на пару вопросов:

* В чем состоит задача кластеризации?
* Для чего применяется кластеризация?


###  Данные (Аресты)

Загружаем данные
```{r, warning=FALSE, message=FALSE}
Arrests <-USArrests
```

Посмотрим на них

```{r, warning=FALSE, message=FALSE}
str(Arrests)
```

*Murder*, *Assault*, *Rape* -- это количество арестов по соответствующим статьям на 100000 жителей.
*UrbanPop* -- доля городского населения в штате.

```{r}
library(ggplot2)
library(GGally)
ggpairs(Arrests)
```

Мы видим какие-то закономерности, например сильную коррелированность Murder и Assault.

Еще одно интересное наблюдение -- бимодальность Assault, и было бы интересно понять, почему так происходит, но это не наша сегодняшняя задача.

Попробуем разделить штаты по похожести.

### Алгоритм k-means

Идея алгоритма очень проста:

1. Возьмем случайным образом k точек и назовем их центрами кластеров. 
2. Для каждой точки найдем ближайший центр и будем считать, что точка пренадлежит этому кластеру.
3. Вычислим новые знацения центров, как среднее арифметическое координат точек приписанных этому кластеру на предыдущем шаге.
4. Будем повторять шаги 2-3 пока центры не перестанут менятся.


Начнем с двух кластеров:

```{r}
km.out=kmeans(Arrests,2,nstart=20)
km.out$cluster
Arrests$clusters = factor(km.out$cluster)
ggpairs(Arrests, mapping = ggplot2::aes(color = clusters), columns = c("Murder", "Assault", "UrbanPop", "Rape"))
```


А теперь 3 кластера:

```{r}
km.out=kmeans(Arrests,3,nstart=1)
km.out$cluster
Arrests$clusters = factor(km.out$cluster)
ggpairs(Arrests, mapping = ggplot2::aes(color = clusters), columns = c("Murder", "Assault", "UrbanPop", "Rape"))
```

Если вы обратите внимание, то лучше всего кластеры разбились по *Assault*. А почему так произошло?

```{r}
scaledArrests <- scale(Arrests[,1:4])
km.out=kmeans(scaledArrests,3,nstart=1)
Arrests$clusters = factor(km.out$cluster)
ggpairs(Arrests, mapping = ggplot2::aes(color = clusters), columns = c("Murder", "Assault", "UrbanPop", "Rape"))
```

Как мы видим, после нормализации данных кластеры довольно сильно изменились.

Это вполне понятно, так как расстояние между объектами вычисляется обычно по формуле $\sqrt{\sum_i (x_i-y_i)^2}$ и если координаты не отмасштабированны, то самая "растянутая" шкала будет важнее всего.

А давайте еще посмотрим, как наши класеры будут выглядеть на графике с главными компонентами в качестве осей.

```{r}
library(caret)
preprocessParams <- preProcess(Arrests, method=c("center", "scale", "pca"))
pcaArrests<-predict(preprocessParams, Arrests)
ggplot()+geom_point(data=pcaArrests, aes(x=PC1, y=PC2,color=clusters))
```

Мы видим, что на главных компонентах разделение выглядит очень понятным.

У k-means есть несколько недостатков:

1. Полученное распределение может быть локальным минимум квадратов расстояний до центра.
2. Результат зависит от выбора исходных центров кластеров.
3. Число кластеров надо знать заранее.


Пример, когда мы получаем два разных разбиения:
```{r}
set.seed(100)
km.out=kmeans(scaledArrests,3,nstart=1)
clusters1 = km.out$cluster
km.out=kmeans(scaledArrests,3,nstart=1)
clusters2 = km.out$cluster
table(clusters1,clusters2)
```


```{r}
ggplot()+geom_point(data=pcaArrests, aes(x=PC1, y=PC2,color=factor(clusters1), shape=factor(clusters2)))
```


### Иерархическая кластеризация

Как мы уже сказали, один из недостатков метода *k-means* -- это необходимость знать число кластеров, плюс некоторая случайность. Давайте посмотрим на метод, который не требует знать число кластеров.

Как мы вам расказывали на лекции, иерахическая кластеризация может строится или на объединении кластеров (агломеративная) или на разбиении. Мы будем работать с первым случаем.

Для того, чтобы выбрать какие кластеры объединять на очередном шаге, мы должны уметь вычислять расстояние не только между точками, но и между кластерами.

Существует много таких способов! 
Самые простые способы определить расстояние между кластерами:

1. Complete Linkage: Расстояние равно самому *большому* расстоянию между точкой одного кластера и точкой другого.
2. Single Linkage: Расстояние равно самому *маленькому* расстоянию между точкой одного кластера и точкой другого.
3. Average Linkage: Расстояние равно *среднему* расстоянию между точками одного кластера и точками другого.
4. Centroid: Расстояние равно расстоянию между центрами кластеров.

Сравним первые три варианта:

```{r}
hc.complete=hclust(dist(scaledArrests), method="complete")
hc.average=hclust(dist(scaledArrests), method="average")
hc.single=hclust(dist(scaledArrests), method="single")
#par(mfrow=c(1,3))
plot(hc.complete,main="Complete Linkage", xlab="", sub="", cex=.9)
plot(hc.average, main="Average Linkage", xlab="", sub="", cex=.9)
plot(hc.single, main="Single Linkage", xlab="", sub="", cex=.9)
```

Как вы видите, мы получили три разных дендрограммы. Какая из них лучше -- зависит от вашей задачи.

Но обычно нам не нужны все кластеры. Имея дендрограммы, мы легко можем получить разбиение на заданное число кластеров, например на 3.

```{r}
hclusters = cutree(hc.complete, 3)
```

Сравним с k-means
```{r}
ggplot()+geom_point(data=pcaArrests, aes(x=PC1, y=PC2,color=factor(clusters2), shape=factor(hclusters)))
```


Есть еще одна функция, которая может быть полезна, -- *heatmap*. Она рисует матрицу данных цветами и добавляет дендрограммы аналогичные иерархической кластеризации.

```{r}
heatmap(as.matrix(scaledArrests),hclustfun = function(x) hclust(x,method = "complete"), scale="none")
```


А вот пример с информацией о генах.

Загрузим данные.
```{r}
library(ISLR)
#?NCI60
nci.labs=NCI60$labs
nci.data=NCI60$data
dim(nci.data)
nci.labs[1:4]
table(nci.labs)
```

*Иерархическая кластеризация*

```{r}
# Clustering the Observations of the NCI60 Data

sd.data=scale(nci.data)
#par(mfrow=c(1,3))
data.dist=dist(sd.data)
plot(hclust(data.dist), labels=nci.labs, main="Complete Linkage", xlab="", sub="",ylab="", cex = 0.6)
plot(hclust(data.dist, method="average"), labels=nci.labs, main="Average Linkage", xlab="", sub="",ylab="", cex = 0.6)
plot(hclust(data.dist, method="single"), labels=nci.labs,  main="Single Linkage", xlab="", sub="",ylab="", cex = 0.6)
hc.out=hclust(dist(sd.data))
hc.clusters=cutree(hc.out,4)
table(hc.clusters,nci.labs)
par(mfrow=c(1,1))
plot(hc.out, labels=nci.labs, cex = 0.6)
abline(h=139, col="red")
hc.out

```


*k-means*

```{r}
set.seed(2)
km.out=kmeans(sd.data, 4, nstart=20)
km.clusters=km.out$cluster
table(km.clusters,hc.clusters)
```



**Теперь ваша очередь**

Сначала маленький классический пример

1. Загрузите датасет Iris
2. Постройте 3 кластера с помошью k-means.
3. Постройте 3 кластера с помошью иерархической кластеризации.
4. Сравните результаты с настоящими значениями сортов.

```{r}
iris
library(ggplot2)
library(GGally)
ggpairs(iris[,1:4])
summary(iris)
```


Если вы все сделали, то загрузите датасет с оценками музыкальных групп и посмотрите, что вы можете сделать с ним.

```{r}
music<-read.csv("~/shared/minor3_2017/3-SecondYear-ML/practice-9-clustering/music.txt")
```


